{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a80576",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25384/2679759559.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustomer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    898\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"http://localhost:9696/predict\"\n",
    "\n",
    "customer = {\n",
    "    \"Administrative\": 3,\n",
    "    \"Administrative_Duration\": 157.4,\n",
    "    \"BounceRates\": 0.036363636,\n",
    "    \"Browser\": 2,\n",
    "    \"ExitRates\": 0.081818182,\n",
    "    \"Informational\": 0,\n",
    "    \"Informational_Duration\": 0.0,\n",
    "    \"Month\": \"Jul\",\n",
    "    \"OperatingSystems\": 3,\n",
    "    \"PageValues\": 0.0,\n",
    "    \"ProductRelated\": 9,\n",
    "    \"ProductRelated_Duration\": 128.5,\n",
    "    \"Region\": 1,\n",
    "    \"SpecialDay\": 0.0,\n",
    "    \"TrafficType\": 3,\n",
    "    \"VisitorType\": \"Returning_Visitor\",\n",
    "    \"Weekend\": True,\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.post(url, json=customer).json()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be02d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DictVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25384/2715758744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mdv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m df_train = pd.DataFrame(\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DictVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import pickle\n",
    "\n",
    "# Loading Data\n",
    "df = pd.read_csv(\"training_data_skf_no_smote.csv\")\n",
    "\n",
    "\n",
    "useful_cols = [col for col in df.columns if col not in [\"id\", \"Revenue\", \"kfold\"]]\n",
    "categorical = [col for col in useful_cols if df[col].dtype in [\"object\", \"bool\"]]\n",
    "numerical = [col for col in useful_cols if col not in categorical]\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=7)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "df_train.Weekend = le.fit_transform(df_train.Weekend)\n",
    "df_test.Weekend = le.transform(df_test.Weekend)\n",
    "\n",
    "\n",
    "dicts = df_train.to_dict(orient=\"records\")\n",
    "dv = DictVectorizer(sparse=False)\n",
    "df_train = pd.DataFrame(\n",
    "    dv.fit_transform(dicts), columns=list(dv.get_feature_names_out())\n",
    ")\n",
    "df_test = pd.DataFrame(dv.transform(dicts), columns=list(dv.get_feature_names_out()))\n",
    "\n",
    "useful_cols = [col for col in df_train.columns if col not in [\"id\", \"Revenue\", \"kfold\"]]\n",
    "categorical = [col for col in useful_cols if df_train[col].dtype in [\"object\", \"bool\"]]\n",
    "numerical = [col for col in useful_cols if col not in categorical]\n",
    "\n",
    "pt = PowerTransformer()\n",
    "pt_num_tr = pd.DataFrame(pt.fit_transform(df_train[useful_cols]), columns=useful_cols)\n",
    "pt_num_ts = pd.DataFrame(pt.transform(df_test[useful_cols]), columns=useful_cols)\n",
    "df_train = pd.concat([df_train.drop(useful_cols, axis=1), pt_num_tr], axis=1)\n",
    "df_test = pd.concat([df_test.drop(useful_cols, axis=1), pt_num_ts], axis=1)\n",
    "\n",
    "useful_cols = [col for col in df_train.columns if col not in [\"id\", \"Revenue\", \"kfold\"]]\n",
    "categorical = [col for col in useful_cols if df_train[col].dtype in [\"object\", \"bool\"]]\n",
    "numerical = [col for col in useful_cols if col not in categorical]\n",
    "scaler = preprocessing.RobustScaler()\n",
    "\n",
    "df_train.Revenue = df_train.Revenue.astype(\"int\")\n",
    "df_train.kfold = df_train.kfold.astype(\"int\")\n",
    "df_test.Revenue = df_test.Revenue.astype(\"int\")\n",
    "df_test.kfold = df_test.kfold.astype(\"int\")\n",
    "\n",
    "\n",
    "# Models\n",
    "params_bg = {\"n_estimators\": 533, \"max_samples\": 32}\n",
    "model_bg = BaggingClassifier(**params_bg, random_state=7)\n",
    "\n",
    "params_lgb = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 47,\n",
    "    \"max_depth\": 17,\n",
    "    \"learning_rate\": 0.331,\n",
    "    \"n_estimators\": 469,\n",
    "    \"reg_alpha\": 30.654408167803027,\n",
    "    \"reg_lambda\": 8.742258358130245,\n",
    "    \"subsample\": 0.45,\n",
    "    \"subsample_freq\": 9,\n",
    "    \"colsample_bytree\": 0.49,\n",
    "    \"min_child_samples\": 26,\n",
    "    \"min_child_weight\": 32,\n",
    "}\n",
    "model_lgb = LGBMClassifier(**params_lgb, random_state=6)\n",
    "\n",
    "\n",
    "params_mlp = {\"alpha\": 0.09631013728513668, \"hidden_layer_sizes\": 7, \"max_iter\": 30}\n",
    "model_mlp = MLPClassifier(**params_mlp, random_state=17, tol=1e-4)\n",
    "\n",
    "\n",
    "params_dt = {\n",
    "    \"max_leaf_nodes\": 6,\n",
    "    \"max_depth\": 254,\n",
    "    \"criterion\": \"entropy\",\n",
    "    \"class_weight\": \"balanced\",\n",
    "}\n",
    "model_dt = DecisionTreeClassifier(**params_dt, random_state=42)\n",
    "\n",
    "\"\"\"### **Final Model: Voting Classifier**\"\"\"\n",
    "\n",
    "scores_train = []\n",
    "scores_valid = []\n",
    "for fold in range(5):\n",
    "    xtrain = df_train[df_train.kfold != fold].reset_index(drop=True)\n",
    "    xvalid = df_train[df_train.kfold == fold].reset_index(drop=True)\n",
    "    xtest = df_test.copy()\n",
    "\n",
    "    ytrain = xtrain.Revenue\n",
    "    yvalid = xvalid.Revenue\n",
    "\n",
    "    xtrain = xtrain[useful_cols]\n",
    "    xvalid = xvalid[useful_cols]\n",
    "    xtest = xtest[useful_cols]\n",
    "\n",
    "    xtrain[numerical] = scaler.fit_transform(xtrain[numerical])\n",
    "    xvalid[numerical] = scaler.transform(xvalid[numerical])\n",
    "    xtest[numerical] = scaler.transform(xtest[numerical])\n",
    "\n",
    "    model_vclf = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"BaggingClassifier\", model_bg),\n",
    "            (\"LightGBM\", model_lgb),\n",
    "            (\"MLPClassifier\", model_mlp),\n",
    "            (\"DecisionTree\", model_dt),\n",
    "        ],\n",
    "        voting=\"hard\",\n",
    "    )\n",
    "    model_vclf.fit(xtrain, ytrain)\n",
    "\n",
    "    preds_valid = model_vclf.predict(xvalid)\n",
    "    preds_train = model_vclf.predict(xtrain)\n",
    "    f1_score_valid = metrics.f1_score(yvalid, preds_valid)\n",
    "    f1_score_train = metrics.f1_score(ytrain, preds_train)\n",
    "    print(f\"Training Acc for fold: {fold}: {model_vclf.score(xtrain,ytrain)}\")\n",
    "    print(f\"Validation Acc for fold: {fold}: {model_vclf.score(xvalid,yvalid)}\")\n",
    "    print(f\"Fold {fold} f1-score-train: \", f1_score_train)\n",
    "    print(f\"Fold {fold} f1-score-Valid: \", f1_score_valid)\n",
    "    scores_train.append(f1_score_train)\n",
    "    scores_valid.append(f1_score_valid)\n",
    "print(np.mean(scores_train), np.std(scores_train))\n",
    "print(np.mean(scores_valid), np.std(scores_valid))\n",
    "\n",
    "\"\"\"## **We can use the final model i.e. ``voting classifier`` for deployment as it has decent mean_f1-score and satisfactory standard deviation.**\"\"\"\n",
    "\n",
    "\"\"\"Save the Bagging CLF model\"\"\"\n",
    "output_file = f\"model_bg.bin\"\n",
    "\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump(model_bg, f_out)\n",
    "\n",
    "print(f\"the model is saved to {output_file}\")\n",
    "\n",
    "\n",
    "\"\"\"Save the LGB model\"\"\"\n",
    "output_file = f\"model_lgb.bin\"\n",
    "\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump(model_lgb, f_out)\n",
    "\n",
    "\n",
    "print(f\"the model is saved to {output_file}\")\n",
    "\n",
    "\n",
    "\"\"\"Save the DT model\"\"\"\n",
    "output_file = f\"model_dt.bin\"\n",
    "\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump(model_dt, f_out)\n",
    "print(f\"the model is saved to {output_file}\")\n",
    "\n",
    "\n",
    "\"\"\"Save the MLP model\"\"\"\n",
    "output_file = f\"model_mlp.bin\"\n",
    "\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump(model_mlp, f_out)\n",
    "print(f\"the model is saved to {output_file}\")\n",
    "\n",
    "\n",
    "\"\"\"Save the Voting CLF model\"\"\"\n",
    "output_file = f\"model_vclf.bin\"\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump(model_vclf, f_out)\n",
    "print(f\"the model is saved to {output_file}\")\n",
    "\n",
    "\n",
    "def preprocess_train(df_train, y_train):\n",
    "\n",
    "    # Label-Encoding boolean variable:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df_train.Weekend = le.fit_transform(df_train.Weekend)\n",
    "\n",
    "    # OHE\n",
    "    dicts = df_train.to_dict(orient=\"records\")\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    df_train = pd.DataFrame(\n",
    "        dv.fit_transform(dicts), columns=list(dv.get_feature_names_out())\n",
    "    )\n",
    "    columns = list(dv.get_feature_names_out())\n",
    "\n",
    "    # PT\n",
    "    pt = PowerTransformer()\n",
    "    pt_num_tr = pd.DataFrame(pt.fit_transform(df_train[columns]), columns=columns)\n",
    "    df_train = pd.concat([df_train.drop(columns, axis=1), pt_num_tr], axis=1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    df_train = scaler.fit_transform(df_train)\n",
    "\n",
    "    # Models\n",
    "    params_bg = {\"n_estimators\": 533, \"max_samples\": 32}\n",
    "    model_bg = BaggingClassifier(**params_bg, random_state=7)\n",
    "\n",
    "    params_lgb = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": 47,\n",
    "        \"max_depth\": 17,\n",
    "        \"learning_rate\": 0.331,\n",
    "        \"n_estimators\": 469,\n",
    "        \"reg_alpha\": 30.654408167803027,\n",
    "        \"reg_lambda\": 8.742258358130245,\n",
    "        \"subsample\": 0.45,\n",
    "        \"subsample_freq\": 9,\n",
    "        \"colsample_bytree\": 0.49,\n",
    "        \"min_child_samples\": 26,\n",
    "        \"min_child_weight\": 32,\n",
    "    }\n",
    "    model_lgb = LGBMClassifier(**params_lgb, random_state=6)\n",
    "\n",
    "    params_mlp = {\"alpha\": 0.09631013728513668, \"hidden_layer_sizes\": 7, \"max_iter\": 30}\n",
    "    model_mlp = MLPClassifier(**params_mlp, random_state=17, tol=1e-4)\n",
    "\n",
    "    params_dt = {\n",
    "        \"max_leaf_nodes\": 6,\n",
    "        \"max_depth\": 254,\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"class_weight\": \"balanced\",\n",
    "    }\n",
    "    model_dt = DecisionTreeClassifier(**params_dt, random_state=42)\n",
    "\n",
    "    # Final Model\n",
    "    model = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"BaggingClassifier\", model_bg),\n",
    "            (\"LightGBM\", model_lgb),\n",
    "            (\"MLPClassifier\", model_mlp),\n",
    "            (\"DecisionTree\", model_dt),\n",
    "        ],\n",
    "        voting=\"hard\",\n",
    "    )\n",
    "    model.fit(df_train, y_train)\n",
    "\n",
    "    return le, dv, pt, scaler, model\n",
    "\n",
    "\n",
    "def predict(df, pt, scaler, dv, model):\n",
    "    df.Weekend = le.transform(df.Weekend)\n",
    "    dicts = df.to_dict(orient=\"records\")\n",
    "    X = dv.transform(dicts)\n",
    "    X = pt.transform(X)\n",
    "    X = scaler.transform(X)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "print(\"Training on created functions:\")\n",
    "# \"\"\"Trying out Model-Prediction on first 20 rows\"\"\"\n",
    "xtrain = df.drop(\"Revenue\", axis=1).copy()\n",
    "ytrain = df.Revenue.copy()\n",
    "le, dv, pt, scaler, model = preprocess_train(xtrain, ytrain)\n",
    "\n",
    "df_q = df.head(20)\n",
    "xtrain = df_q.drop([\"Revenue\", \"kfold\"], axis=1)\n",
    "ytrain = df_q.Revenue\n",
    "preds_train = predict(xtrain, pt, scaler, dv, model)\n",
    "print([(i, j) for i, j in zip(ytrain, preds_train)][:20])\n",
    "\n",
    "output_file = f\"model_final.bin\"\n",
    "with open(output_file, \"wb\") as f_out:\n",
    "    pickle.dump((le, dv, pt, scaler, model), f_out)\n",
    "\n",
    "print(f\"the model is saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30fcd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(sparse=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7a61aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DictVectorizer' object has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25384/1890649036.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DictVectorizer' object has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "dv.__version__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ac0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
